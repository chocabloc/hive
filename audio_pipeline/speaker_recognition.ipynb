{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSbL1hcDGU_h"
   },
   "source": [
    "# Speaker Recognition\n",
    "We try to achieve two things:\n",
    "\n",
    "\n",
    "*   Given an audio recording of a conversation, get a diarized version of it. That is get a transcript of the form:  \n",
    "\\[timestamp\\]: Speaker 1: ....  \n",
    "\\[timestamp\\]: Speaker 2: ....\n",
    "*   However, the above transcript doesn't give us who the speakers are. We use pre-existing voice samples to deduce this. It is done in two parts and uses the *Speechbrain Encoder*  which gives an embedding for an audio sample:\n",
    "  1. We embed the pre-existing samples we have.\n",
    "  2. We use the timestamps obtained from the diarizer to concatenate fragments of the test audio where a single speaker is speaking. We compare the embedding of this concatenation with the embeddings of the samples to match unknown speakers to known ones.  \n",
    "\n",
    "Remark: the following code is written for google colab and you might notice artefacts of the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install speechbrain pydub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from speechbrain.inference import EncoderClassifier\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de1526ce7464612b8ab8bcd8560179c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered parameter transfer hook for _load\n",
      "/usr/local/lib/python3.11/dist-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load_if_possible\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Fetching files for pretraining (no collection directory set)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ed1ea41a604602a4e519d5c6ae0c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding_model.ckpt:   0%|          | 0.00/83.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/embedding_model.ckpt\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42d4cda6d61419189c39aea7aaef02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mean_var_norm_emb.ckpt:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/mean_var_norm_emb.ckpt\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838be1ef2ae141f9bee53081a91eb670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "classifier.ckpt:   0%|          | 0.00/5.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/classifier.ckpt\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3c164a45094292a5729864f66088da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "label_encoder.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/embedding_model.ckpt\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/mean_var_norm_emb.ckpt\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/classifier.ckpt\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n",
      "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt\n"
     ]
    }
   ],
   "source": [
    "classifier=EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    run_opts={\"device\":DEVICE}\n",
    ")\n",
    "\n",
    "def embed_audiosegment(audio_seg,max_len_sec=5):\n",
    "    samples = np.array(audio_seg.get_array_of_samples()).astype(np.float32)\n",
    "\n",
    "    # Normalize to [-1, 1]\n",
    "    samples /= np.iinfo(audio_seg.array_type).max\n",
    "\n",
    "    # If stereo -> average channels\n",
    "    if audio_seg.channels > 1:\n",
    "        samples = samples.reshape((-1, audio_seg.channels))\n",
    "        samples = samples.mean(axis=1)\n",
    "\n",
    "    # Convert to torch tensor shape [1, time]\n",
    "    waveform = torch.tensor(samples).unsqueeze(0)\n",
    "\n",
    "    # Resample if needed\n",
    "    if audio_seg.frame_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=audio_seg.frame_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # We only use 5 seconds of the sample\n",
    "    max_samples = max_len_sec * 16000\n",
    "    waveform = waveform[:, :max_samples]\n",
    "\n",
    "    # Relative length (full audio = 1.0)\n",
    "    lengths = torch.tensor([1.0])\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = classifier.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu()\n",
    "    return speaker_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXwu6BhuOxcn"
   },
   "source": [
    "# Disclaimer\n",
    "Use the audio sample I have given at your own risk. It is some random podcast from my feed and is vulgar and political. To use your examples:  \n",
    "1. Store the audio samples of known people at audio_pipline/samples/\n",
    "2. Store the test audio in audio_pipeline/tests/  \n",
    "\n",
    "and make necessary changes in the code. Its better to use your own test examples so that we can check the robustness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1186776306.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  speaker_embeddings = classifier.encode_batch(torch.tensor(waveform))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded known speakers: ['smitha', 'anand', 'sushant', 'abhijit']\n"
     ]
    }
   ],
   "source": [
    "# Dictionary containing names with audio samples of known people.\n",
    "known_speakers={\n",
    "    'smitha':\"/content/drive/MyDrive/AIML/project/audio_pipeline/samples/smitha.wav\",\n",
    "    'anand':\"/content/drive/MyDrive/AIML/project/audio_pipeline/samples/anand.wav\",\n",
    "    'sushant':\"/content/drive/MyDrive/AIML/project/audio_pipeline/samples/sushant.wav\",\n",
    "    'abhijit':\"/content/drive/MyDrive/AIML/project/audio_pipeline/samples/abhijit.wav\"\n",
    "}\n",
    "known_embeddings={}\n",
    "\n",
    "for name,path in known_speakers.items():\n",
    "  seg=AudioSegment.from_file(path)\n",
    "  known_embeddings[name]=embed_audiosegment(seg)\n",
    "\n",
    "print(\"Loaded known speakers:\",list(known_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install assemblyai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import assemblyai as aai\n",
    "aai.settings.api_key = \"YOUR API KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using Assembly AI to get the diarized version of our audio sample. See: https://www.assemblyai.com/\n",
    "# Remember to change the path below if you are using your own test example.\n",
    "audio_fn=\"/content/drive/MyDrive/AIML/project/audio_pipeline/tests/sample6.wav\"\n",
    "audio=AudioSegment.from_file(audio_fn)\n",
    "upload_url = aai.Transcriber().upload_file(audio_fn)\n",
    "config = aai.TranscriptionConfig(speaker_labels=True)\n",
    "\n",
    "transcriber = aai.Transcriber()\n",
    "transcript = transcriber.transcribe(upload_url, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1186776306.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  speaker_embeddings = classifier.encode_batch(torch.tensor(waveform))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster A → sushant (cos=0.671)\n",
      "Cluster B → abhijit (cos=0.668)\n",
      "Cluster C → smitha (cos=0.756)\n",
      "Cluster D → anand (cos=0.706)\n",
      "Cluster E → Unknown_E (cos=0.356)\n"
     ]
    }
   ],
   "source": [
    "# The fragments of the test audio having the same speaker (obtained by using the timestamps from the diarizer) are of different durations.\n",
    "# We want to weight them by their durations. Clearly, shorter segments must get lesser weight.\n",
    "# The function below, takes embeddings (basically, a list of vectors) and weights for each of them to spit out their normalized weighted average.\n",
    "\n",
    "def weighted_mean_embeddings(embs,weights):\n",
    "  if len(embs)==1:\n",
    "    return embs[0]\n",
    "  W=torch.tensor(weights,dtype=torch.float32).unsqueeze(1)\n",
    "  M=torch.stack(embs,dim=0)\n",
    "  avg=(W*M).sum(dim=0)/(W.sum()+1e-12)\n",
    "  return avg/(avg.norm(p=2)+1e-12)\n",
    "\n",
    "# The cluster_segs dictionary stores the speaker label given by the diarizer (like Speaker 1, Speaker 2, etc.) along with the corresponding segments (and their durations)\n",
    "# the speaker has spoken in.\n",
    "cluster_segs={}\n",
    "for utt in transcript.utterances:\n",
    "  seg=audio[utt.start:utt.end]\n",
    "  dur=max(0.001,(utt.end-utt.start)/1000.0)\n",
    "  cluster_segs.setdefault(utt.speaker,[]).append((seg,dur))\n",
    "\n",
    "# The cluster_embeddings dictionary has the speaker label and the corresponding test audio embedding for the speaker. For example {Speaker 1: corresponding embedding}\n",
    "cluster_embeddings={}\n",
    "for label,segs in cluster_segs.items():\n",
    "  embs,weights=[],[]\n",
    "  for seg,dur in segs:\n",
    "    embs.append(embed_audiosegment(seg))\n",
    "    weights.append(dur)\n",
    "  cluster_embeddings[label]=weighted_mean_embeddings(embs,weights)\n",
    "\n",
    "# UNKNOWN_THRESHOLD is a hyperparameter I am yet to optimize. It is the threshold similarity score for two speakers to be classified as the same.\n",
    "# We compare the similarity of two embeddings by their cosine similarity score. This is just a fancy name for - dotproduct(a,b)/(norm(a)*norm(b))\n",
    "UNKNOWN_THRESHOLD=0.55\n",
    "label_to_name={}\n",
    "for label,emb in cluster_embeddings.items():\n",
    "  best_name,best_score=None,-1.0\n",
    "  for name,kemb in known_embeddings.items():\n",
    "    score=F.cosine_similarity(emb,kemb,dim=0).item()\n",
    "    if score>best_score:\n",
    "      best_name,best_score=name,score\n",
    "  if best_score>=UNKNOWN_THRESHOLD:\n",
    "    label_to_name[label]=best_name\n",
    "  else:\n",
    "    label_to_name[label]=f\"Unknown_{label}\"\n",
    "  print(f\"Cluster {label} → {label_to_name[label]} (cos={best_score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Named Diarization ===\n",
      "[0.40-35.75] sushant: So let me, let me. The closest parallel to this, if at all parallels can be\n",
      "drawn is in the 19th century when the British were fighting this menace not just in the Indian\n",
      "subcontinent but they were fighting it in Africa and Sudan and other places. And what they did was.\n",
      "You probably cannot do that today. But the kind of massacres that the British did at that point of\n",
      "time, they eliminated them. They would put them in front of cannons and blow them. Today you will\n",
      "have the entire world on you if we were to make an example of these jihadis like that.\n",
      "[35.75-43.91] abhijit: Winston Churchill was using gas in Iraq. British army used gas in Iraq. The\n",
      "British invented concentration camps in the Boer War and they used disease as a weapon.\n",
      "[43.91-44.39] sushant: Exactly.\n",
      "[44.39-45.23] abhijit: They slotted.\n",
      "[45.23-47.27] sushant: They taught the Germans to build concentration.\n",
      "[47.27-49.31] abhijit: Camps after Kitchener was killed in.\n",
      "[49.95-51.27] sushant: But genius they British.\n",
      "[51.27-56.31] abhijit: Then Gordon was killed in Khartoum. What Kitchener did in Sudan was nothing\n",
      "less.\n",
      "[58.07-72.15] sushant: But under Congress watch, every year there used to be a major riot, right?\n",
      "But they were a secular party or secular party. Secularism.\n",
      "[73.83-80.51] abhijit: Add to that, you know, this is what upsets me and Rahul. Upset, upsets me.\n",
      "[81.95-85.87] sushant: Sonia, Congress party, cow slaughter, laws, Manmohan.\n",
      "[86.11-117.48] abhijit: Narasimara, Rahul Gandhi, Indira Gandhi never upset me because they, they\n",
      "maintained the congress is a big tent party. I think the decline started in the Sonia years when\n",
      "they started going to see she's a European with these European social democrat ideas about this\n",
      "thing. And unfortunately Rahul in his mind is much more. Look at who he respects. He can never\n",
      "respect a Ghansham or Harlal from son of the Soil. He needs only Ayvika Ivy League ka chhapala\n",
      "people to surround him.\n",
      "[117.56-120.60] smitha: I don't think so now. Yeah he does now he has all these leftist.\n",
      "[121.64-124.92] abhijit: He has Kanaya Kumar but Kanaya Kumar is Desi iv.\n",
      "[125.24-129.08] smitha: No, there was a time that there were only the elite around us.\n",
      "[129.64-134.52] abhijit: This is what gets me about here. One second, let me finish.\n",
      "[140.18-142.34] smitha: Just two more sentences and then we go to Anand.\n",
      "[144.58-158.78] abhijit: And I think this is where Abhijit gets it wrong that he extrapolates Rahul\n",
      "Gandhi's mindset onto Nehru and Indira Gandhi. They were nothing. They were watch. I can never\n",
      "question the nationalism of Nehru and Indira Gandhi.\n",
      "[158.78-161.06] sushant: Oh my God.\n",
      "[163.46-164.98] abhijit: This is a hill I'm willing to die.\n",
      "[165.60-167.12] smitha: This is the hill I'm willing to die on.\n",
      "[167.52-187.36] abhijit: And that is why I say this. When I say this you have to understand my\n",
      "bitterness now with the Congress that you know, Bilawal is a kind of gay, twinky, androgynous\n",
      "version of Rahul Gandhi. And Rahul Gandhi is the Butch, macho, straight version of his boyfriend.\n",
      "[187.68-188.80] sushant: I knew his boyfriend.\n",
      "[189.04-191.69] smitha: You knew Bilal's boyfriend? Did he.\n",
      "[191.77-194.41] abhijit: Did you know him like Esther knew the King?\n",
      "[194.41-195.69] smitha: Or did he visit?\n",
      "[215.29-223.48] abhijit: Such a fantastic job. Prc. You know. You know, Priyanka can sometimes, you\n",
      "know.\n",
      "[223.48-224.68] smitha: She'S very good at mimicking.\n",
      "[225.16-230.28] anand: But the good point is you didn't say what you said in English. Did he come in\n",
      "the midnight?\n",
      "[230.92-233.88] Unknown_E: That's exactly what who comes in the night?\n",
      "[234.36-247.72] smitha: Who let the dogs out. Let's get on to this. Elon Musk.\n",
      "[263.22-297.58] anand: Israel problem solved. Because visceral problem. Pakistan visceral problem.\n",
      "India is a visceral problem. Stone Age maybe. Pakistan, you understand? They might not have the\n",
      "means that they have today of attacking. But they will send in swarms and swarms of. You understand\n",
      "that the population of Gaza has now increased in the last two years, right?\n",
      "[297.58-298.62] abhijit: It has. Yes.\n",
      "[299.18-308.50] anand: Is it so they are playing by numbers you cannot defeat. That's all I'm\n",
      "limited point I was trying to make. If we believe that we have ebbed this problem. We have. That is\n",
      "all I'm saying.\n",
      "[308.50-310.52] Unknown_E: Nobody believes the problem is solved. Odd.\n",
      "[310.52-311.72] smitha: But what we trying to say.\n",
      "[313.24-315.64] Unknown_E: The price has to be increasingly a lot.\n",
      "[317.00-321.80] anand: You are contradicting Shashan Paji when he says General that they don't give\n",
      "a toss about.\n",
      "[326.12-330.36] sushant: By attacking their airports, by hitting their command and control centers.\n",
      "[333.16-345.14] Unknown_E: He says what do they care about their assets? You are hitting that. The\n",
      "generals want to go to the uk. The generals want to go to Dubai. You're spoiling that for them.\n",
      "There are. So I think the cost has to be increased.\n",
      "[345.94-354.58] smitha: When you're talking about egg cheeseburger, Zaru Bataniya. When he's talking\n",
      "about Gaza Ki population has increased despite the bombing. This Zakiyur Rehman Laqvi.\n",
      "[354.98-356.02] sushant: He was procreating.\n",
      "[357.86-361.06] smitha: The guy was in jail and his wife was kept on producing children.\n",
      "[369.69-370.73] abhijit: Isn't Zakir Rehman?\n",
      "[371.77-372.09] sushant: No.\n",
      "[372.09-373.61] smitha: And despite of that is producing children.\n",
      "[373.61-374.85] anand: Before or after he went to kid?\n",
      "[374.85-376.09] smitha: Aren't you ashamed of yourself?\n",
      "[376.17-377.77] Unknown_E: Let's do this in English ones.\n",
      "[378.41-380.73] abhijit: Don't you know? Don't you know biology is bigotry.\n",
      "[381.53-382.17] smitha: Oh God.\n",
      "[383.61-384.81] abhijit: So you are a biologist?\n",
      "[384.81-385.01] smitha: No.\n",
      "[385.01-388.64] abhijit: Which means you're a bigger ticket. Bygot.\n",
      "[388.80-389.52] smitha: By God.\n",
      "[389.68-391.20] abhijit: By God. You're a bygot.\n",
      "[397.44-398.72] smitha: What was the need to do this?\n",
      "[405.20-407.44] anand: He shot himself in the foot. He's done a Rahul Gandhi out here.\n",
      "[407.44-407.68] Unknown_E: Okay.\n",
      "[407.68-420.02] smitha: So those who are just listening to it and not watching it. It's Elon Musk's\n",
      "tweet, which he did. We said Time to drop the bomb. The real Donald Trump is in Epstein files. That\n",
      "is the real reason that they have not been made.\n",
      "[420.02-421.38] sushant: But you have taken this out of context.\n",
      "[421.86-487.84] smitha: Yeah. So somebody wants to explain how. So between Trump and Musk, the fight\n",
      "started and then finally it came to this level where he did this and then there's no going back.\n",
      "Musk benefited from the Democrats and not just benefit analysis. It's not just me, it's the\n",
      "Democrats who are saying it. It's the late night comedians who are saying it. It's the news media in\n",
      "America which is saying it. That Trump had no hesitation in taking Musk's money when he was there\n",
      "and he agreed to all the low budget and you curtail government spending, all that. And now when he\n",
      "doesn't agree to it, he brings Sam Altman there, asks this guy to go tariffs. He doesn't agree to\n",
      "what Musk is. He brought initially Musk in for. He just evicts Musk from the tp. I'm not justifying.\n",
      "Listen to what I'm saying. When he does this. And then Musk, whether he's justified or not in\n",
      "throwing a tantrum, a hissy fit on Twitter, on X in what he did, the whole thing just unraveled\n",
      "crazily in a couple of hours. And this was the final straw.\n",
      "[487.84-488.60] Unknown_E: I think he's deleted.\n",
      "[488.60-489.08] sushant: But it was.\n",
      "[489.88-491.24] abhijit: It's deleted, not deleted.\n",
      "[491.24-491.96] anand: Are you serious?\n",
      "[491.96-493.04] Unknown_E: Yeah, he's deleted it.\n",
      "[493.04-494.44] abhijit: Look at it.\n",
      "[495.56-499.72] Unknown_E: He deleted it yesterday when the lawyer came out saying that no, the.\n",
      "[499.72-502.76] smitha: He sent the file, deleted his post.\n",
      "[502.84-508.68] Unknown_E: Is it? Yeah, yeah. See, I'm telling you. So the lawyers came out, we've\n",
      "seen the papers and Trump's name is not there.\n",
      "[508.68-509.24] sushant: So you have to.\n",
      "[509.56-522.76] smitha: Cash Patel was on Joe Rogan's show. Cash Patel said that nothing of this\n",
      "sort has happened. The Joe Rogan show was amazing because it happened live while this thing was\n",
      "blowing up on the Joe Rogan.\n",
      "[522.76-547.98] sushant: Okay. So my thing is that this, the fight was not immediate. It was\n",
      "building up Sam Altman and other things were also part of that buildup. But I personally think it\n",
      "started because Elon Musk made the big mistake of not keeping some distance between him and Donald\n",
      "Trump after having helped him. He was constantly next to Donald Trump and then he was, you know,\n",
      "it's okay to do it.\n"
     ]
    }
   ],
   "source": [
    "# We are finally done! Prints the transcript substituting names of speakers we are certain about.\n",
    "import textwrap\n",
    "print(\"\\n=== Named Diarization ===\")\n",
    "for utt in transcript.utterances:\n",
    "    who = label_to_name.get(utt.speaker, f\"Unknown_{utt.speaker}\")\n",
    "    print(textwrap.fill(f\"[{utt.start/1000:.2f}-{utt.end/1000:.2f}] {who}: {utt.text}\",width=100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
