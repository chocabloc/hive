{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPxMRopyyVyc3/faSVDJ1OW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eiMNz0V3BMht"},"outputs":[],"source":["#all imports and initializations\n","\n","import tiktoken\n","from openai import OpenAI\n","from sentence_transformers import SentenceTransformer,util\n","from transformers import AutoTokenizer\n","import torch\n","import textwrap\n","import re\n","import json\n","import uuid\n","from datetime import date\n","\n","client = OpenAI(\n","    base_url=\"https://router.huggingface.co/v1\",\n","    api_key=\"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\",\n",")\n","\n","model=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", model_max_length=1000000)\n"]},{"cell_type":"code","source":["#all helper functions can be found here\n","################################################################################\n","\n","def chunk_transcript(transcript: str, model_max_tokens: int = 4096, reserved_tokens: int = 512):\n","    enc = tiktoken.get_encoding(\"cl100k_base\")\n","    tokens = enc.encode(transcript)\n","    chunk_size = model_max_tokens - reserved_tokens\n","    chunks = []\n","    for i in range(0, len(tokens), chunk_size):\n","        chunk_tokens = tokens[i:i + chunk_size]\n","        chunk_text = enc.decode(chunk_tokens)\n","        chunks.append(chunk_text)\n","    return chunks\n","\n","################################################################################\n","\n","def extract_entities(transcript: str):\n","    chunks = chunk_transcript(transcript)\n","    results = []\n","    for i, chunk in enumerate(chunks, 1):\n","        completion = client.chat.completions.create(\n","            model=\"mistralai/Mistral-7B-Instruct-v0.2:featherless-ai\",\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"Extract entities and information from the following transcript. Leave lists/dictionaries empty if the conversation doesnt have them.\n","\n","Output format must strictly follow:\n","\n","persons: [list of people mentioned]\n","locations: [list of locations mentioned]\n","dates: {{\"date\": \"context where date appears\"}}\n","events: [list of events mentioned]\n","action_items: {{\"task/to-do\": person it is entrusted to}}\n","\n","For action_items, include all tasks implied or explicitly mentioned that require effort, planning, or follow-up. For action_items keep person as \"unknown\", if it is unclear whom it is assigned to and keep the tasks slightly detailed (4-5 words)\n","By dates, i mean all sorts of mentions in the transcript - normalized ones (eg: dd-mm-yy), explicit ones (eg: 10th of august, the 8th (here you will have to guess the month by context)), or relative ones(eg: next friday)\n","If a date is mentioned without month/year (e.g., ‘the 12th’), still include it in the output with the exact wording used.\n","If relative dates are used (e.g., ‘next Tuesday’), include them as they are.\n","Remember the following at all costs:\n","Output only in the format shown below. Do not include explanations, transcripts, or extra text. Do not include comments.\n","{chunk}\"\"\"\n","                }\n","            ],\n","          temperature=0.1\n","        )\n","        results.append(completion.choices[0].message.content)\n","    return results\n","\n","################################################################################\n","\n","def normalize_output(text: str) -> str:\n","    # squash everything into one line, clean spacing\n","    return \" \".join(text.split())\n","\n","def parse_llm_output(text: str) -> dict:\n","    text = normalize_output(text)\n","\n","    data = {}\n","\n","    persons_match = re.search(r\"persons:\\s*(\\[.*?\\])\", text)\n","    locations_match = re.search(r\"locations:\\s*(\\[.*?\\])\", text)\n","    dates_match = re.search(r\"dates:\\s*({.*?})\", text)\n","    events_match = re.search(r\"events:\\s*(\\[.*?\\])\", text)\n","    action_items_match = re.search(r\"action_items:\\s*({.*?})\", text)\n","\n","    if persons_match:\n","        data[\"persons\"] = json.loads(persons_match.group(1))\n","    if locations_match:\n","        data[\"locations\"] = json.loads(locations_match.group(1))\n","    if dates_match:\n","        data[\"dates\"] = json.loads(dates_match.group(1))\n","    if events_match:\n","        data[\"events\"] = json.loads(events_match.group(1))\n","    if action_items_match:\n","      data[\"action_items\"] = json.loads(action_items_match.group(1))\n","\n","    return data\n","\n","def merge_results(parsed_list):\n","    merged = {\"persons\": [], \"locations\": [], \"dates\": {}, \"events\": [], \"action_items\": {}}\n","    for chunk in parsed_list:\n","        merged[\"persons\"].extend(chunk.get(\"persons\", []))\n","        merged[\"locations\"].extend(chunk.get(\"locations\", []))\n","        merged[\"events\"].extend(chunk.get(\"events\", []))\n","        merged[\"dates\"].update(chunk.get(\"dates\", {}))\n","\n","        # merge action_items dicts\n","        for task, person in chunk.get(\"action_items\", {}).items():\n","            # keep latest assignment if duplicates\n","            merged[\"action_items\"][task] = person\n","\n","    # deduplicate lists\n","    merged[\"persons\"] = list(set(merged[\"persons\"]))\n","    merged[\"locations\"] = list(set(merged[\"locations\"]))\n","    merged[\"events\"] = list(set(merged[\"events\"]))\n","\n","    return merged\n","\n","################################################################################\n","\n","def parse_conversation(text, min_words=5):\n","    lines = text.strip().splitlines()\n","    conv = []\n","    current_speaker, current_dialogue = None, []\n","\n","    for line in lines:\n","        if \":\" in line:\n","            # new speaker line\n","            speaker, dialogue = line.split(\":\", 1)\n","            if current_speaker and \" \".join(current_dialogue).strip():\n","                # save previous\n","                full_dialogue = \" \".join(current_dialogue).strip()\n","                if len(full_dialogue.split()) >= min_words:\n","                    conv.append((current_speaker.strip(), full_dialogue))\n","            # start new\n","            current_speaker = speaker.strip()\n","            current_dialogue = [dialogue.strip()]\n","        else:\n","            # continuation of current speaker's dialogue\n","            if current_speaker:\n","                current_dialogue.append(line.strip())\n","\n","    # add last dialogue\n","    if current_speaker and \" \".join(current_dialogue).strip():\n","        full_dialogue = \" \".join(current_dialogue).strip()\n","        if len(full_dialogue.split()) >= min_words:\n","            conv.append((current_speaker.strip(), full_dialogue))\n","\n","    return conv\n","\n","################################################################################\n","\n","def chunk_text(text, chunk_size=1000, overlap=200):\n","    \"\"\"Split text into overlapping chunks by tokens.\"\"\"\n","    tokens = tokenizer.encode(text)\n","    chunks = []\n","    for i in range(0, len(tokens), chunk_size - overlap):\n","        chunk = tokens[i:i + chunk_size]\n","        chunks.append(tokenizer.decode(chunk))\n","    return chunks\n","\n","def summarize_chunk(chunk):\n","    \"\"\"Summarize one chunk using Mistral.\"\"\"\n","    completion = client.chat.completions.create(\n","        model=\"mistralai/Mistral-7B-Instruct-v0.2:featherless-ai\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes conversations.\"},\n","            {\"role\": \"user\", \"content\": f\"Summarize this conversation without omitting any important names, dates mentioned:\\n\\n{chunk}\"}\n","        ],\n","        max_tokens=200,\n","        temperature=0.1\n","    )\n","    return completion.choices[0].message.content\n","\n","def full_summary(chunks):\n","  summaries=[]\n","  for i, chunk in enumerate(chunks):\n","    summary=summarize_chunk(chunk)\n","    summaries.append(summary)\n","  return summaries\n","\n","################################################################################\n","\n","def important_dialogues(conversation_text, summaries, threshold=0.5):\n","    summary_embs=model.encode(summaries,convert_to_tensor=True,normalize_embeddings=True)\n","    conv = parse_conversation(conversation_text)\n","\n","    important = []\n","    for speaker, dialogue in conv:\n","        d_emb = model.encode(dialogue, convert_to_tensor=True, normalize_embeddings=True)\n","        sims = util.cos_sim(d_emb, summary_embs)\n","        max_sim = torch.max(sims).item()\n","        if max_sim >= threshold:\n","            important.append((speaker, dialogue,d_emb))\n","    summs=[]\n","    for i in range(len(summaries)):\n","        summs.append((summaries[i],summary_embs[i]))\n","    return important,summs\n","\n","################################################################################\n","\n","\n"],"metadata":{"id":"1B0nqqZSB1rJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_json(transcript):\n","  chunks=chunk_text(transcript)\n","  summaries=full_summary(chunks)\n","  important,summs=important_dialogues(transcript,summaries)\n","\n","  extracted=extract_entities(transcript)\n","  parsed=[parse_llm_output(out) for out in extracted]\n","  metadata=merge_results(parsed)\n","\n","  dat=str(date.today())\n","  summary_dict={}\n","  for text,embedding in summs:\n","    uid=uuid.uuid4().hex[:8]\n","    summary_dict[f\"{dat}_s_{uid}\"]={\n","        \"text\":text,\n","        \"embedding\":embedding\n","    }\n","\n","  dialogue_dict={}\n","  for speaker,dialogue,embedding in important:\n","    uid=uuid.uuid4().hex[:8]\n","    dialogue_dict[f\"{dat}_d_{uid}\"]={\n","        \"speaker\":speaker,\n","        \"text\":dialogue,\n","        \"embedding\":embedding\n","    }\n","\n","  final_json={\n","      \"metadata\":metadata,\n","      \"summary\":summary_dict,\n","      \"important_dialogues\":dialogue_dict\n","  }\n","\n","  return final_json"],"metadata":{"id":"k3_BjEiyrUyR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"test2.txt\", \"r\", encoding=\"utf-8\") as f:\n","      transcript = f.read()\n","\n","final=generate_json(transcript)\n"],"metadata":{"id":"wSlQDxg3_mHs","executionInfo":{"status":"ok","timestamp":1755840090752,"user_tz":-330,"elapsed":29633,"user":{"displayName":"Muralidhar Rao","userId":"13644060075979080195"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["print(len(final[\"important_dialogues\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qMTUw7aDHNA","executionInfo":{"status":"ok","timestamp":1755840158390,"user_tz":-330,"elapsed":21,"user":{"displayName":"Muralidhar Rao","userId":"13644060075979080195"}},"outputId":"fc5574db-d4ad-4569-c696-4369e4438545"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n"]}]}]}