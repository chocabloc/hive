{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9pOoGeyvR6s2zlTijGDof"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!python -m spacy download en_core_web_trf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XETdNopcjzc8","executionInfo":{"status":"ok","timestamp":1755694911271,"user_tz":-330,"elapsed":36868,"user":{"displayName":"Muralidhar Rao","userId":"13644060075979080195"}},"outputId":"a111bdf0-a9da-49a3-e375-0db01a6caaae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-trf==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting spacy-curated-transformers<1.0.0,>=0.2.2 (from en-core-web-trf==3.8.0)\n","  Downloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl.metadata (2.7 kB)\n","Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n","  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n","Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n","  Downloading curated_tokenizers-0.0.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n","Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.8.0+cu126)\n","Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.12/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.11.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.14.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.0.2)\n","Downloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl (237 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.9/237.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading curated_tokenizers-0.0.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (734 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.0/734.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, en-core-web-trf\n","Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.8.0 spacy-curated-transformers-0.3.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_trf')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ej7vq7AGWCsQ"},"outputs":[],"source":["import spacy\n","import json"]},{"cell_type":"code","source":["from typing import Text\n","nlp = spacy.load(\"en_core_web_lg\")\n","\n","with open(\"test.txt\", \"r\", encoding=\"utf-8\") as f:\n","      text = f.read()\n","\n","doc = nlp(text)\n","\n","# buckets for entities\n","entities = {\n","    \"persons\": set(),\n","    \"organizations\": set(),\n","    \"locations\": set(),\n","    \"gpes\": set(),\n","    \"norp\": set(),\n","    \"dates\": set()\n","}\n","\n","for ent in doc.ents:\n","    if ent.label_ == \"PERSON\":\n","        entities[\"persons\"].add(ent.text)\n","    elif ent.label_ == \"ORG\":\n","        entities[\"organizations\"].add(ent.text)\n","    elif ent.label_ == \"LOC\":\n","        entities[\"locations\"].add(ent.text)\n","    elif ent.label_ == \"GPE\":\n","        entities[\"gpes\"].add(ent.text)\n","    elif ent.label_ == \"NORP\":\n","        entities[\"norp\"].add(ent.text)\n","    elif ent.label_ == \"DATE\":\n","        entities[\"dates\"].add(ent.text)\n","\n","# convert sets to lists for JSON serialization\n","entities = {k: list(v) for k, v in entities.items()}\n","\n","print(json.dumps(entities, indent=2))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"scFAxcgaWcvH","executionInfo":{"status":"ok","timestamp":1755694947669,"user_tz":-330,"elapsed":7610,"user":{"displayName":"Muralidhar Rao","userId":"13644060075979080195"}},"outputId":"53d022fb-be75-4635-8637-2b636d431a13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"persons\": [\n","    \"Kanaya Kumar\",\n","    \"Gordon\",\n","    \"Bilal\",\n","    \"Esther\",\n","    \"Abhijit\",\n","    \"Musk\",\n","    \"Zakir Rehman\",\n","    \"Elon Musk's\",\n","    \"Joe Rogan's\",\n","    \"Gaza Ki\",\n","    \"Shashan Paji\",\n","    \"Joe Rogan\",\n","    \"Donald Trump\",\n","    \"Sam Altman\",\n","    \"Anand\",\n","    \"sushant\",\n","    \"Rahul\",\n","    \"Bilawal\",\n","    \"Donald\\nTrump\",\n","    \"Rahul\\nGandhi\",\n","    \"Winston Churchill\",\n","    \"Rahul Gandhi\",\n","    \"Zaru Bataniya\",\n","    \"Indira Gandhi\",\n","    \"Narasimara\",\n","    \"Butch\",\n","    \"Elon Musk\",\n","    \"abhijit\",\n","    \"Epstein\",\n","    \"Sonia\",\n","    \"Priyanka\",\n","    \"Cash Patel\"\n","  ],\n","  \"organizations\": [\n","    \"Congress\",\n","    \"Kitchener\",\n","    \"Twitter\",\n","    \"Trump and Musk\",\n","    \"Trump\",\n","    \"congress\",\n","    \"This Zakiyur Rehman Laqvi\",\n","    \"Time\"\n","  ],\n","  \"locations\": [\n","    \"Soil\",\n","    \"Unknown_E\",\n","    \"Africa\"\n","  ],\n","  \"gpes\": [\n","    \"Khartoum\",\n","    \"Kitchener\",\n","    \"Pakistan\",\n","    \"Nehru\",\n","    \"Sudan\",\n","    \"Harlal\",\n","    \"Iraq\",\n","    \"Bygot\",\n","    \"America\",\n","    \"Ghansham\",\n","    \"Dubai\",\n","    \"India\",\n","    \"uk\",\n","    \"Prc\",\n","    \"Israel\",\n","    \"Manmohan\",\n","    \"Gaza\"\n","  ],\n","  \"norp\": [\n","    \"Indian\",\n","    \"European\",\n","    \"Germans\",\n","    \"Democrats\",\n","    \"British\",\n","    \"democrat\"\n","  ],\n","  \"dates\": [\n","    \"every year\",\n","    \"today\",\n","    \"Today\",\n","    \"the last two years\",\n","    \"the 19th century\",\n","    \"yesterday\"\n","  ]\n","}\n"]}]},{"cell_type":"code","source":["from openai import OpenAI\n","\n","# Hugging Face router\n","client = OpenAI(\n","    base_url=\"https://router.huggingface.co/v1\",\n","    api_key=\"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\",\n",")\n","\n","transcript=(\"\"\"sushant: So let me, let me. The closest parallel to this, if at all parallels can be\n","drawn is in the 19th century when the British were fighting this menace not just in the Indian\n","subcontinent but they were fighting it in Africa and Sudan and other places. And what they did was.\n","You probably cannot do that today. But the kind of massacres that the British did at that point of\n","time, they eliminated them. They would put them in front of cannons and blow them. Today you will\n","have the entire world on you if we were to make an example of these jihadis like that.\n","abhijit: Winston Churchill was using gas in Iraq. British army used gas in Iraq. The\n","British invented concentration camps in the Boer War and they used disease as a weapon.\n","sushant: Exactly.\n","abhijit: They slotted.\n","sushant: They taught the Germans to build concentration.\n","abhijit: Camps after Kitchener was killed in.\n","sushant: But genius they British.\n","abhijit: Then Gordon was killed in Khartoum. What Kitchener did in Sudan was nothing\n","less.\n","sushant: But under Congress watch, every year there used to be a major riot, right?\n","But they were a secular party or secular party. Secularism.\n","abhijit: Add to that, you know, this is what upsets me and Rahul. Upset, upsets me.\n","sushant: Sonia, Congress party, cow slaughter, laws, Manmohan.\n","abhijit: Narasimara, Rahul Gandhi, Indira Gandhi never upset me because they, they\n","maintained the congress is a big tent party. I think the decline started in the Sonia years when\n","they started going to see she's a European with these European social democrat ideas about this\n","thing. And unfortunately Rahul in his mind is much more. Look at who he respects. He can never\n","respect a Ghansham or Harlal from son of the Soil. He needs only Ayvika Ivy League ka chhapala\n","people to surround him.\n","smitha: I don't think so now. Yeah he does now he has all these leftist.\n","abhijit: He has Kanaya Kumar but Kanaya Kumar is Desi iv.\n","smitha: No, there was a time that there were only the elite around us.\n","abhijit: This is what gets me about here. One second, let me finish.\n","smitha: Just two more sentences and then we go to Anand.\n","abhijit: And I think this is where Abhijit gets it wrong that he extrapolates Rahul\n","Gandhi's mindset onto Nehru and Indira Gandhi. They were nothing. They were watch. I can never\n","question the nationalism of Nehru and Indira Gandhi.\n","sushant: Oh my God.\n","abhijit: This is a hill I'm willing to die.\n","smitha: This is the hill I'm willing to die on.\"\"\")\n","\n","prompt = f\"\"\"\n","Extract entities and information from the following transcript of a conversation. Leave lists/dictionaries empty if the conversation doesnt have them\n","\n","Output format must strictly follow:\n","\n","persons: [list of people mentioned]\n","locations: [list of locations mentioned]\n","dates: {{ \"date\": \"context where date appears\" }}\n","events: [list of events mentioned]\n","action_items: [list of action items or tasks to take]\n","\n","i.e. standard python list/dictionary formats\n","\n","Transcript:\n","{transcript}\n","\"\"\"\n","\n","completion = client.chat.completions.create(\n","    model=\"mistralai/Mistral-7B-Instruct-v0.2:featherless-ai\",\n","    messages=[{\"role\": \"user\", \"content\": prompt}],\n","    temperature=0\n",")\n","\n","result = completion.choices[0].message.content.strip()\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFnteVrBlaoE","executionInfo":{"status":"ok","timestamp":1755696086169,"user_tz":-330,"elapsed":10455,"user":{"displayName":"Muralidhar Rao","userId":"13644060075979080195"}},"outputId":"1b47a59a-173c-44dc-924e-e99a921396f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["persons: [\"Sushant\", \"Abhijit\", \"Winston Churchill\", \"Kitchener\", \"Gordon\", \"Sonia\", \"Congress party\", \"Manmohan Singh\", \"Narasimha Rao\", \"Rahul Gandhi\", \"Indira Gandhi\", \"Ghansham\", \"Harlal\", \"Ivy League\", \"Kanaya Kumar\"]\n","locations: [\"Indian subcontinent\", \"Africa\", \"Sudan\", \"Iraq\", \"Boer War\", \"Khartoum\"]\n","dates: {}\n","events: [\"British fighting a menace in the Indian subcontinent and Africa\", \"British using gas in Iraq\", \"British inventing concentration camps in the Boer War\", \"British using disease as a weapon\", \"Kitchener being killed in Sudan\", \"Every year major riots under Congress watch\"]\n","action_items: []\n"]}]},{"cell_type":"code","source":["import tiktoken\n","from openai import OpenAI\n","client = OpenAI(\n","    base_url=\"https://router.huggingface.co/v1\",\n","    api_key=\"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\",\n",")\n","\n","def chunk_transcript(transcript: str, model_max_tokens: int = 4096, reserved_tokens: int = 512):\n","    enc = tiktoken.get_encoding(\"cl100k_base\")\n","    tokens = enc.encode(transcript)\n","    chunk_size = model_max_tokens - reserved_tokens\n","    chunks = []\n","    for i in range(0, len(tokens), chunk_size):\n","        chunk_tokens = tokens[i:i + chunk_size]\n","        chunk_text = enc.decode(chunk_tokens)\n","        chunks.append(chunk_text)\n","    return chunks\n","\n","def extract_entities(transcript: str):\n","    chunks = chunk_transcript(transcript)\n","    results = []\n","    for i, chunk in enumerate(chunks, 1):\n","        completion = client.chat.completions.create(\n","            model=\"mistralai/Mistral-7B-Instruct-v0.2:featherless-ai\",\n","            messages=[\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"Extract entities and information from the following transcript. Leave lists/dictionaries empty if the conversation doesnt have them.\n","\n","Output format must strictly follow:\n","\n","persons: [list of people mentioned]\n","locations: [list of locations mentioned]\n","dates: {{\"date\": \"context where date appears\"}}\n","events: [list of events mentioned]\n","action_items: {{\"task/to-do\": person it is entrusted to}}\n","\n","For action_items, include all tasks implied or explicitly mentioned that require effort, planning, or follow-up. For action_items keep person as \"unknown\", if it is unclear whom it is assigned to and keep the tasks slightly detailed (4-5 words)\n","By dates, i mean all sorts of mentions in the transcript - normalized ones (eg: dd-mm-yy), explicit ones (eg: 10th of august, the 8th (here you will have to guess the month by context)), or relative ones(eg: next friday)\n","If a date is mentioned without month/year (e.g., ‘the 12th’), still include it in the output with the exact wording used.\n","If relative dates are used (e.g., ‘next Tuesday’), include them as they are.\n","Remember the following at all costs:\n","Output only in the format shown below. Do not include explanations, transcripts, or extra text. Do not include comments.\n","{chunk}\"\"\"\n","                }\n","            ],\n","          temperature=0.1\n","        )\n","        results.append(completion.choices[0].message.content)\n","    return results\n","\n","if __name__ == \"__main__\":\n","    transcript=(\"\"\"Sam: Okay team, let’s align. The client wants the first draft ready by October 15th.\n","Nina: Ugh, that’s right after my brother’s graduation on the 13th. Guess I’ll be working late nights.\n","Raj: Haha, nothing new there, Nina. You practically live in the office.\n","Nina: Says the guy who once slept under his desk during the March deadline.\n","Leo: Oh, I remember that — Raj was snoring so loudly that security thought there was a raccoon in the building.\n","Raj: Lies. Pure slander.\n","\n","Sam: Anyway, focus — after the draft, we’ve got the board presentation on November 3rd.\n","Nina: That’s the same week as Diwali, isn’t it?\n","Leo: Yeah, my family’s pushing for a get-together on the 5th. I’ll have to juggle both.\n","Aisha: My cousin’s engagement is also that weekend, on the 6th. Everyone’s getting married except me.\n","Raj: Don’t worry, we’ll put you on the next company dating app pilot. Launch date: “pending.”\n","Sam: Guys, please.\n","\n","Nina: Fine, fine. What about the product demo? Wasn’t it on December 10th?\n","Sam: Tentatively. But the marketing team wants to shift it earlier, maybe to the 1st.\n","Leo: Classic marketing — always promising things before we’re ready.\n","Raj: If they move it, I swear I’m calling in sick.\n","Nina: You say that every time.\n","\n","Sam: Also, don’t forget our team outing. HR booked something for December 18th.\n","Leo: Wait, isn’t that close to your anniversary, Raj?\n","Raj: Uh… yeah, the 20th. Thanks for reminding me, I should probably book something before my wife kills me.\n","Nina: Haha, better prioritize that over the outing then.\n","\n","Aisha: So to summarize — draft due Oct 15, presentation Nov 3, demo Dec 1 or 10, outing Dec 18. And Raj’s survival deadline is Dec 20.\n","Sam: Perfect. Meeting adjourned\"\"\")\n","    outputs = extract_entities(transcript)\n","    for i, out in enumerate(outputs,1):\n","        print(f\"\\n--- Chunk {i} ---\\n{out}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Zf5dc_7qTkG","executionInfo":{"status":"ok","timestamp":1755839505061,"user_tz":-330,"elapsed":11252,"user":{"displayName":"Muralidhar Rao","userId":"13644060075979080195"}},"outputId":"19654eb0-43e8-4380-b3df-b73a88a5ff3a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Chunk 1 ---\n"," persons: [\"Sam\", \"Nina\", \"Raj\", \"Leo\", \"Aisha\"]\n","locations: []\n","dates: {\"October 15th\": \"client wants first draft ready\", \"13th\": \"Nina's brother's graduation\", \"November 3rd\": \"board presentation\", \"Diwali\": \"mentioned as conflicting event\", \"December 1st or 10th\": \"product demo\", \"December 18th\": \"team outing\", \"December 20th\": \"Raj's anniversary\"}\n","events: [\"client wants first draft ready\", \"Nina's brother's graduation\", \"board presentation\", \"Diwali\", \"product demo\", \"team outing\"]\n","action_items: {\n","  \"finish first draft\": \"unknown\",\n","  \"prepare for board presentation\": \"unknown\",\n","  \"attend brother's graduation\": \"Nina\",\n","  \"juggle Diwali and team presentation\": \"Leo\",\n","  \"attend cousin's engagement\": \"Aisha\",\n","  \"attend team outing\": \"unknown\",\n","  \"book something for anniversary\": \"Raj\"\n","}\n","\n"]}]},{"cell_type":"code","source":["import ast\n","import json\n","\n","import re\n","import json\n","\n","def normalize_output(text: str) -> str:\n","    # squash everything into one line, clean spacing\n","    return \" \".join(text.split())\n","\n","def parse_llm_output(text: str) -> dict:\n","    text = normalize_output(text)\n","\n","    data = {}\n","\n","    persons_match = re.search(r\"persons:\\s*(\\[.*?\\])\", text)\n","    locations_match = re.search(r\"locations:\\s*(\\[.*?\\])\", text)\n","    dates_match = re.search(r\"dates:\\s*({.*?})\", text)\n","    events_match = re.search(r\"events:\\s*(\\[.*?\\])\", text)\n","    action_items_match = re.search(r\"action_items:\\s*({.*?})\", text)\n","\n","    if persons_match:\n","        data[\"persons\"] = json.loads(persons_match.group(1))\n","    if locations_match:\n","        data[\"locations\"] = json.loads(locations_match.group(1))\n","    if dates_match:\n","        data[\"dates\"] = json.loads(dates_match.group(1))\n","    if events_match:\n","        data[\"events\"] = json.loads(events_match.group(1))\n","    if action_items_match:\n","      data[\"action_items\"] = json.loads(action_items_match.group(1))\n","\n","    return data\n","\n","def merge_results(parsed_list):\n","    merged = {\"persons\": [], \"locations\": [], \"dates\": {}, \"events\": [], \"action_items\": {}}\n","    for chunk in parsed_list:\n","        merged[\"persons\"].extend(chunk.get(\"persons\", []))\n","        merged[\"locations\"].extend(chunk.get(\"locations\", []))\n","        merged[\"events\"].extend(chunk.get(\"events\", []))\n","        merged[\"dates\"].update(chunk.get(\"dates\", {}))\n","\n","        # merge action_items dicts\n","        for task, person in chunk.get(\"action_items\", {}).items():\n","            # keep latest assignment if duplicates\n","            merged[\"action_items\"][task] = person\n","\n","    # deduplicate lists\n","    merged[\"persons\"] = list(set(merged[\"persons\"]))\n","    merged[\"locations\"] = list(set(merged[\"locations\"]))\n","    merged[\"events\"] = list(set(merged[\"events\"]))\n","\n","    return merged\n","\n","if __name__ == \"__main__\":\n","    llm_outputs = [\n","        \"\"\"persons: [\"Alice\", \"Bob\", \"Sarah\", \"Emily\", \"John\", \"Mike\", \"Rachel\"]\n","locations: [\"Seattle\", \"Boston\", \"San Francisco\", \"New York\", \"Berlin\", \"Paris\", \"Greece\", \"Chicago\", \"Goa\", \"Bangalore\"]\n","dates: {\n"," \"2019\": \"Seattle concert\",\n"," \"2019-Year Next\": \"Sarah planning move to San Francisco\",\n"," \"2020-April\": \"John's birthday party\",\n"," \"2020-Summer\": \"Emily's planned trip to Japan\",\n"," \"2020-December\": \"Real holiday party at Mike's house\",\n"," \"2021-December\": \"Alice going to the conference in Berlin\",\n"," \"2022-March\": \"Bob attending climate summit in Paris\",\n"," \"2025-June\": \"Rachel planning group vacation to Greece\",\n"," \"2025-July\": \"Emily's wedding\",\n"," \"2025-August\": \"Company retreat in Goa\",\n"," \"2025-September\": \"Product launch event in Bangalore\"\n","}\n","events: [\"Seattle concert\", \"Coldplay show\", \"Zoom birthday party\", \"Real holiday party\", \"conference in Berlin\", \"climate summit\", \"group vacation to Greece\", \"Emily's wedding\", \"company retreat\", \"product launch event\"]\n","action_items: {\n"," \"Booking flights\": \"Alice\",\n"," \"Looking into hotels\": \"Bob\",\n"," \"Handling local tours\": \"Sarah\",\n"," \"Preparing small presentation on yearly goals\": \"Unknown\"\n","}\"\"\"\n","    ]\n","\n","    parsed = [parse_llm_output(out) for out in llm_outputs]\n","    final_json = merge_results(parsed)\n","\n","    print(json.dumps(final_json, indent=2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOell7RKrCXi","executionInfo":{"status":"ok","timestamp":1755836082804,"user_tz":-330,"elapsed":33,"user":{"displayName":"Muralidhar Rao","userId":"13644060075979080195"}},"outputId":"071c0578-83a5-4017-8018-ecc141ed78ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"persons\": [\n","    \"Bob\",\n","    \"Alice\",\n","    \"Mike\",\n","    \"Sarah\",\n","    \"Emily\",\n","    \"John\",\n","    \"Rachel\"\n","  ],\n","  \"locations\": [\n","    \"Chicago\",\n","    \"Greece\",\n","    \"San Francisco\",\n","    \"Goa\",\n","    \"Bangalore\",\n","    \"Paris\",\n","    \"New York\",\n","    \"Boston\",\n","    \"Berlin\",\n","    \"Seattle\"\n","  ],\n","  \"dates\": {\n","    \"2019\": \"Seattle concert\",\n","    \"2019-Year Next\": \"Sarah planning move to San Francisco\",\n","    \"2020-April\": \"John's birthday party\",\n","    \"2020-Summer\": \"Emily's planned trip to Japan\",\n","    \"2020-December\": \"Real holiday party at Mike's house\",\n","    \"2021-December\": \"Alice going to the conference in Berlin\",\n","    \"2022-March\": \"Bob attending climate summit in Paris\",\n","    \"2025-June\": \"Rachel planning group vacation to Greece\",\n","    \"2025-July\": \"Emily's wedding\",\n","    \"2025-August\": \"Company retreat in Goa\",\n","    \"2025-September\": \"Product launch event in Bangalore\"\n","  },\n","  \"events\": [\n","    \"product launch event\",\n","    \"Real holiday party\",\n","    \"conference in Berlin\",\n","    \"group vacation to Greece\",\n","    \"Emily's wedding\",\n","    \"company retreat\",\n","    \"climate summit\",\n","    \"Coldplay show\",\n","    \"Seattle concert\",\n","    \"Zoom birthday party\"\n","  ],\n","  \"action_items\": {\n","    \"Booking flights\": \"Alice\",\n","    \"Looking into hotels\": \"Bob\",\n","    \"Handling local tours\": \"Sarah\",\n","    \"Preparing small presentation on yearly goals\": \"Unknown\"\n","  }\n","}\n"]}]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer,util\n","from transformers import AutoTokenizer\n","import torch\n","import textwrap\n","\n","model_name=\"sentence-transformers/all-mpnet-base-v2\"\n","model=SentenceTransformer(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", model_max_length=1000000)\n","\n","import re\n","\n","def parse_conversation(text, min_words=5):\n","    lines = text.strip().splitlines()\n","    conv = []\n","    current_speaker, current_dialogue = None, []\n","\n","    for line in lines:\n","        if \":\" in line:\n","            # new speaker line\n","            speaker, dialogue = line.split(\":\", 1)\n","            if current_speaker and \" \".join(current_dialogue).strip():\n","                # save previous\n","                full_dialogue = \" \".join(current_dialogue).strip()\n","                if len(full_dialogue.split()) >= min_words:\n","                    conv.append((current_speaker.strip(), full_dialogue))\n","            # start new\n","            current_speaker = speaker.strip()\n","            current_dialogue = [dialogue.strip()]\n","        else:\n","            # continuation of current speaker's dialogue\n","            if current_speaker:\n","                current_dialogue.append(line.strip())\n","\n","    # add last dialogue\n","    if current_speaker and \" \".join(current_dialogue).strip():\n","        full_dialogue = \" \".join(current_dialogue).strip()\n","        if len(full_dialogue.split()) >= min_words:\n","            conv.append((current_speaker.strip(), full_dialogue))\n","\n","    return conv\n","\n","def important_dialogues(conversation_text, summaries, threshold=0.5):\n","    summary_embs=model.encode(summaries,convert_to_tensor=True,normalize_embeddings=True)\n","    conv = parse_conversation(conversation_text)\n","\n","    important = []\n","    for speaker, dialogue in conv:\n","        d_emb = model.encode(dialogue, convert_to_tensor=True, normalize_embeddings=True)\n","        sims = util.cos_sim(d_emb, summary_embs)\n","        max_sim = torch.max(sims).item()\n","        if max_sim >= threshold:\n","            important.append((speaker, dialogue, max_sim))\n","    # Sort by similarity descending\n","    important.sort(key=lambda x: x[2], reverse=True)\n","    return important\n","\n","CHUNK_SIZE = 1000   # tokens per chunk\n","OVERLAP = 200       # overlap between chunks\n","\n","def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):\n","    \"\"\"Split text into overlapping chunks by tokens.\"\"\"\n","    tokens = tokenizer.encode(text)\n","    chunks = []\n","    for i in range(0, len(tokens), chunk_size - overlap):\n","        chunk = tokens[i:i + chunk_size]\n","        chunks.append(tokenizer.decode(chunk))\n","    return chunks\n","\n","def summarize_chunk(chunk):\n","    \"\"\"Summarize one chunk using Mistral.\"\"\"\n","    completion = client.chat.completions.create(\n","        model=\"mistralai/Mistral-7B-Instruct-v0.2:featherless-ai\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes conversations.\"},\n","            {\"role\": \"user\", \"content\": f\"Summarize this conversation without omitting any important names, dates mentioned:\\n\\n{chunk}\"}\n","        ],\n","        max_tokens=200,\n","        temperature=0.1\n","    )\n","    return completion.choices[0].message.content\n","\n","def full_summary(chunks):\n","  summaries=[]\n","  for i, chunk in enumerate(chunks):\n","    summary=summarize_chunk(chunk)\n","    summaries.append(summary)\n","  return summaries\n","\n","with open(\"test.txt\", \"r\", encoding=\"utf-8\") as f:\n","      transcript = f.read()\n","chunks = chunk_text(transcript)\n","print(f\"Transcript split into {len(chunks)} chunks.\\n\")\n","summaries=full_summary(chunks)\n","important=important_dialogues(transcript,summaries)\n","\n","for speaker, dialogue, score in important:\n","    print(textwrap.fill(f\"[{speaker}] ({score:.2f}): {dialogue}\",width=100))\n","    print('-'*100)"],"metadata":{"id":"MQMSx18Q4-0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["important_dialogues={}\n","embedded_summaries={}\n","\n","for diag in important:\n","  important_dialogues[diag]=model.encode(diag,convert_to_tensor=True,normalize_embedding=True)\n","\n","for summ in summaries:\n","  embedded_summaries[summ]=model.encode(summ,convert_to_tensor=True,normalize_embedding=True)\n","\n","def generate_json(metadata,embedded_summaries,important_dialogues):\n","  final_json={\n","      \"metadata\":metadata,\n","      \"summary\":embedded_summaries,\n","      \"important_dialogues\":important_dialogues\n","  }\n","  return final_json\n","\n"],"metadata":{"id":"M-fcXrai77gw"},"execution_count":null,"outputs":[]}]}