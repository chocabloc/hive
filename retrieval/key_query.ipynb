{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3482e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "from parsedatetime import Calendar\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from spacy.matcher import Matcher\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0f33f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "kw_model = KeyBERT()\n",
    "cal = Calendar()\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "49e7dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define patterns for event-like noun phrases\n",
    "# This rule looks for an optional determiner/preposition followed by a noun\n",
    "# E.g., \"the meeting,\" \"in the call,\" \"a discussion\"\n",
    "event_pattern = [{\"POS\": {\"IN\": [\"DET\", \"ADP\", \"PROPN\"]}, \"OP\": \"?\"}, \n",
    "                 {\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"GENERAL_EVENT\", [event_pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cbcfa466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structured_info(text):\n",
    "    \"\"\"\n",
    "    Extracts structured information (persons, locations, dates, events, action items)\n",
    "    from a given text using spaCy and KeyBERT. It now also resolves relative dates.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # Initialize dictionaries and lists for structured output\n",
    "    persons = set()\n",
    "    locations = set()\n",
    "    dates = {}\n",
    "    events = set()\n",
    "    action_items = {}\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        if nlp.vocab.strings[match_id] == \"GENERAL_EVENT\":\n",
    "            events.add(span.text)\n",
    "\n",
    "    # Use spaCy to find named entities and resolve dates\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            persons.add(ent.text)\n",
    "        elif ent.label_ == \"GPE\" or ent.label_ == \"LOC\":\n",
    "            locations.add(ent.text)\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            phrase = ent.text\n",
    "            time_struct, parse_status = cal.parse(phrase)\n",
    "            \n",
    "            if parse_status:\n",
    "                actual_datetime = datetime.datetime(*time_struct[:6])\n",
    "                actual_date_str = actual_datetime.strftime('%Y-%m-%d')\n",
    "                \n",
    "                dates[phrase] = {\n",
    "                    \"actual_date\": actual_date_str,\n",
    "                    \"context\": text\n",
    "                }\n",
    "            else:\n",
    "                # Fallback for dates that parsedatetime can't handle\n",
    "                dates[phrase] = {\"context\": text}\n",
    "        elif ent.label_ == \"EVENT\":\n",
    "            events.add(ent.text)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        # Basic filter: check if the matched span is not a named entity\n",
    "        # and has a length of 1 or 2 tokens to avoid capturing too much\n",
    "        is_entity = any(ent.text == span.text for ent in doc.ents)\n",
    "        if not is_entity and len(span) <= 2:\n",
    "            events.add(span.text)\n",
    "\n",
    "    # Use spaCy's dependency parser to find action items\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            subject = None\n",
    "            task = None\n",
    "            \n",
    "            for child in token.children:\n",
    "                if child.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
    "                    subject = child.text\n",
    "            \n",
    "            task_tokens = [token.text]\n",
    "            for child in token.children:\n",
    "                if child.dep_ in (\"dobj\", \"compound\", \"prt\"):\n",
    "                    task_tokens.append(child.text)\n",
    "            \n",
    "            if subject and len(task_tokens) > 1:\n",
    "                task_str = \" \".join(task_tokens)\n",
    "                action_items[task_str] = subject\n",
    "            elif subject and not task:\n",
    "                action_items[token.text] = subject\n",
    "    \n",
    "    sentiment_dict = sentiment_analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment_dict['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        sentiment_label = \"positive\"\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment_label = \"negative\"\n",
    "    else:\n",
    "        sentiment_label = \"neutral\"\n",
    "    \n",
    "    # Prepare the final output in the requested format\n",
    "    if any(persons) or any(locations) or any(dates) or any(events) or any(action_items) or any(sentiment_label):\n",
    "        # Return the structured data if found\n",
    "        return {\n",
    "            \"persons\": list(persons),\n",
    "            \"locations\": list(locations),\n",
    "            \"dates\": dates,\n",
    "            \"events\": list(events),\n",
    "            \"action_items\": action_items,\n",
    "            \"sentiment\": {\n",
    "            \"label\": sentiment_label,\n",
    "            \"score\": compound_score\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        # Fallback: Use KeyBERT to get general keywords\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            text,\n",
    "            keyphrase_ngram_range=(1, 2),\n",
    "            stop_words=\"english\",\n",
    "            top_n=5\n",
    "        )\n",
    "        return {\n",
    "            \"status\": \"No specific structured data found.\",\n",
    "            \"keywords\": [keyword[0] for keyword in keywords]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b780ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'persons': [], 'locations': [], 'dates': {}, 'events': ['project', 'guy', 'the guy'], 'action_items': {'said': 'who', 'help me': 'he'}, 'sentiment': {'label': 'positive', 'score': 0.4019}}\n"
     ]
    }
   ],
   "source": [
    "expan_query = \"Tell me about the guy who said he would help me with my project.\"\n",
    "extracted_data = extract_structured_info(expan_query)\n",
    "print(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a80fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIDs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
